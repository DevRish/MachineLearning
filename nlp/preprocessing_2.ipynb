{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af88d27b-c48c-494e-bedb-d19ecc05acfe",
   "metadata": {},
   "source": [
    "# <b><u>PREPROCESSING II: SENTENCE TO VECTOR and WORD TO VECTOR</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6df509-dd4d-4427-a29e-de62689aadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import copy\n",
    "import re\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcba4db-2b26-475c-92a3-bfe1c58027db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def processing1(sentence):\n",
    "    sentence = re.sub('[^a-zA-Z0-9\\s]', '', sentence) # remove punctuations and any kind of symbols\n",
    "    sentence = sentence.lower() # convert everything to same case to avoid redundancy due to mixed cases\n",
    "    sentence = sentence.split() # list of words\n",
    "    sentence = [word for word in sentence if word not in set(nltk.corpus.stopwords.words('english')) ] # remove unimportant stopwords\n",
    "    # sentence = [stemmer.stem(word) for word in sentence ] # stem to base words\n",
    "    sentence = [lemmatizer.lemmatize(word) for word in sentence ] # lemmatize to base words\n",
    "    sentence = ' '.join(sentence) # back to sentence from processed list of words\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa64469-5699-4a7c-b545-ccadf6d3a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "I have three visions for India. \n",
    "In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds. \n",
    "From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.\n",
    "Why? Because we respect the freedom of others.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd54d9b-cf95-46f3-a937-d16e341ce7a7",
   "metadata": {},
   "source": [
    "## <b><u>Extracting sentences and Preprocessing I (cleanup): </u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab651eb-c21f-4b76-85c4-ce5e5af185ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence\n",
       "0                                                                     three vision india\n",
       "1             3000 year history people world come invaded u captured land conquered mind\n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took\n",
       "3                                                                        yet done nation\n",
       "4                                                                       conquered anyone\n",
       "5                                    grabbed land culture history tried enforce way life\n",
       "6                                                                                       \n",
       "7                                                                 respect freedom others"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all sentences from paragraph. Each sentence will be turned into vector having some specific features (unique words).\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Processing 1\n",
    "sentences = [processing1(sentence) for sentence in sentences]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Preprocessed Sentence\"] = sentences\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f042155-ad51-4bc3-8a16-f4b198ab7de0",
   "metadata": {},
   "source": [
    "## <b><u>ONE HOT ENCODING (sentence to vector based on EXISTENCE of words)</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5d076-ca38-4761-924e-59454ee6930d",
   "metadata": {},
   "source": [
    "In one hot encoding(OHE) technique, we **consider WORDS as FEATURES (each unique word present in the entire corpus(collection of sentences) is treated as a unique feature)** <br>\n",
    "Each sentence is represented by a vector **based on the \"EXISTENCE\" of each of the unique words(features) in the sentence**\n",
    "<br><br>\n",
    "For Example: <br>\n",
    "Let `corpus = [ \"orange is a fruit of orange colour\", \"carrot is a vegetable of orange colour\" ]` <br>\n",
    "After cleanup and preprocessing 1, <br>\n",
    "`corpus = [ \"orange fruit orange colour\", \"carrot vegetable orange colour\" ]` <br>\n",
    "So; **unique words** in sorted order are: `[ \"carrot\", \"colour\", \"fruit\", \"orange\", \"vegetable\" ]` <br>\n",
    "So; after vectorizing using **one hot encoding** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 0 1 1 1 0 ], [ 1 1 0 1 1 ] ]` <br><br>\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can represent sentences as vectors for training purpose\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Semantics(grammatical relationships) and any other relationships among the words are not taken into consideration, each word is an independant feature, independant of other words.\n",
    "- No weightage is given to the words, every word is given equal importance and each word is considered independant and equally related to every other word in the corpus's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94b1e90-f524-4a54-a77b-9e2006b8b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features(Words) are:\n",
      "\n",
      "['x0_3000' 'x0_alexander' 'x0_anyone' 'x0_british' 'x0_came' 'x0_captured'\n",
      " 'x0_come' 'x0_conquered' 'x0_culture' 'x0_done' 'x0_dutch' 'x0_enforce'\n",
      " 'x0_freedom' 'x0_french' 'x0_grabbed' 'x0_greek' 'x0_history' 'x0_india'\n",
      " 'x0_invaded' 'x0_land' 'x0_life' 'x0_looted' 'x0_mind' 'x0_mogul'\n",
      " 'x0_nation' 'x0_onwards' 'x0_others' 'x0_people' 'x0_portuguese'\n",
      " 'x0_respect' 'x0_three' 'x0_took' 'x0_tried' 'x0_turk' 'x0_u' 'x0_vision'\n",
      " 'x0_way' 'x0_world' 'x0_year' 'x0_yet']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "      <th>One Hot Encoded Vector Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence  \\\n",
       "0                                                                     three vision india   \n",
       "1             3000 year history people world come invaded u captured land conquered mind   \n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took   \n",
       "3                                                                        yet done nation   \n",
       "4                                                                       conquered anyone   \n",
       "5                                    grabbed land culture history tried enforce way life   \n",
       "6                                                                                          \n",
       "7                                                                 respect freedom others   \n",
       "\n",
       "                                                                                                                                                                      One Hot Encoded Vector Representation  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]  \n",
       "2  [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "4  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting sentences to vectors using One Hot Encoding strategy\n",
    "ohe = sklearn.preprocessing.OneHotEncoder()\n",
    "\n",
    "vocab = []\n",
    "for sentence in sentences:\n",
    "    vocab = vocab + sentence.split()\n",
    "\n",
    "vocab = list(set(vocab)) # remove duplicates\n",
    "\n",
    "ohe.fit([ [word] for word in vocab ])\n",
    "\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in sentence.split():\n",
    "        vector = vector + ohe.transform([[word]]).toarray()[0]\n",
    "    vectors.append(vector.tolist())\n",
    "\n",
    "print(\"Features(Words) are:\\n\")\n",
    "print(ohe.get_feature_names_out())\n",
    "\n",
    "df[\"One Hot Encoded Vector Representation\"] = vectors\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ee710-a44d-4af3-8ae8-e47681324f77",
   "metadata": {},
   "source": [
    "## <b><u>BAG OF WORDS (sentence to vector based on FREQUENCY of words) (CountVectorizer)</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba388-f250-444a-87fd-5385a9849d5d",
   "metadata": {},
   "source": [
    "In bag of words technique(BOW), we **consider WORDS as FEATURES (each unique word present in the entire corpus(collection of sentences) is treated as a unique feature)** <br>\n",
    "Each sentence is represented by a vector **based on the \"FREQUENCY\" of each of the unique words(features) in the sentence**\n",
    "<br><br>\n",
    "For Example: <br>\n",
    "Let `corpus = [ \"orange is a fruit of orange colour\", \"carrot is a vegetable of orange colour\" ]` <br>\n",
    "After cleanup and preprocessing 1, <br>\n",
    "`corpus = [ \"orange fruit orange colour\", \"carrot vegetable orange colour\" ]` <br>\n",
    "So; **unique words** in sorted order are: `[ \"carrot\", \"colour\", \"fruit\", \"orange\", \"vegetable\" ]` <br>\n",
    "So; after vectorizing using **bag of words(frequency based)** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 0 1 1 2 0 ], [ 1 1 0 1 1 ] ]` <br><br>\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can represent sentences as vectors for training purpose\n",
    "- Gives different weightage to different words, so they are not treated the same.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Semantics(grammatical relationships) and any other relationships among the words are not taken into consideration, each word is an independant feature, independant of other words.\n",
    "- Weightage is given on basis of frequency in the sentence itself (kindof local scope). So, each sentence is independant, no relation is considered between different sentences in entire corpus. So; the weightage given to the different words is not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a507fa3-6bfa-4326-9e12-c9846741131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features(Words) are:\n",
      "\n",
      "{   '3000': 0,\n",
      "    'alexander': 1,\n",
      "    'anyone': 2,\n",
      "    'british': 3,\n",
      "    'came': 4,\n",
      "    'captured': 5,\n",
      "    'come': 6,\n",
      "    'conquered': 7,\n",
      "    'culture': 8,\n",
      "    'done': 9,\n",
      "    'dutch': 10,\n",
      "    'enforce': 11,\n",
      "    'freedom': 12,\n",
      "    'french': 13,\n",
      "    'grabbed': 14,\n",
      "    'greek': 15,\n",
      "    'history': 16,\n",
      "    'india': 17,\n",
      "    'invaded': 18,\n",
      "    'land': 19,\n",
      "    'life': 20,\n",
      "    'looted': 21,\n",
      "    'mind': 22,\n",
      "    'mogul': 23,\n",
      "    'nation': 24,\n",
      "    'onwards': 25,\n",
      "    'others': 26,\n",
      "    'people': 27,\n",
      "    'portuguese': 28,\n",
      "    'respect': 29,\n",
      "    'three': 30,\n",
      "    'took': 31,\n",
      "    'tried': 32,\n",
      "    'turk': 33,\n",
      "    'vision': 34,\n",
      "    'way': 35,\n",
      "    'world': 36,\n",
      "    'year': 37,\n",
      "    'yet': 38}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "      <th>Bag of words Vector Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "      <td>[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence  \\\n",
       "0                                                                     three vision india   \n",
       "1             3000 year history people world come invaded u captured land conquered mind   \n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took   \n",
       "3                                                                        yet done nation   \n",
       "4                                                                       conquered anyone   \n",
       "5                                    grabbed land culture history tried enforce way life   \n",
       "6                                                                                          \n",
       "7                                                                 respect freedom others   \n",
       "\n",
       "                                                                                      Bag of words Vector Representation  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1  [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]  \n",
       "2  [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "4  [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]  \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"One Hot Encoded Vector Representation\"], axis=1, inplace=True)\n",
    "\n",
    "# Converting sentences to vectors using Bag Of Words strategy\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(sentences).toarray().tolist()\n",
    "\n",
    "print(\"Features(Words) are:\\n\")\n",
    "pp.pprint(vectorizer.vocabulary_)\n",
    "\n",
    "df[\"Bag of words Vector Representation\"] = vectors\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4433da55-25f7-4c37-8514-c40567f17f9e",
   "metadata": {},
   "source": [
    "## <b><u>BAG OF N-GRAMS (sentence to vector based on frequency of N-GRAMS(group of N CONSECUTIVE words)</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34c3452-5990-4177-8a75-fa4c91627465",
   "metadata": {},
   "source": [
    "In bag of n-grams(BON) technique, we **consider N-GRAMS(groups of CONSECUTIVE words) as FEATURES (each sequence of consecutive words of specified sizes, extracted from every sentence in the corpus(collection of sentences), is treated as an unique feature is treated as a unique feature)** <br>\n",
    "Each sentence is represented by a vector **based on the \"FREQUENCY\" of each of the unique N-GRAMS(group of words=> treated asfeatures here) in the sentence** <br>\n",
    "**Words are Uni-grams**, **Bi-grams** are groups of 2 words each, **Tri-grams** are groups of 3 words each, and so on... <br>\n",
    "<br>\n",
    "For Example: <br>\n",
    "Let `corpus = [ \"orange is a fruit of orange colour\", \"carrot is a vegetable of orange colour\" ]` <br>\n",
    "After cleanup and preprocessing 1, <br>\n",
    "`corpus = [ \"orange fruit orange colour\", \"carrot vegetable orange colour\" ]` <br>\n",
    "So; all **unigrams** are: `[ \"carrot\", \"colour\", \"fruit\", \"orange\", \"vegetable\" ]` <br>\n",
    "So; all **bigrams** are: `[ \"orange fruit\", \"fruit orange\", \"orange colour\", \"carrot vegetable\", \"vegetable orange\" ]` <br>\n",
    "So; all **trigrams** are: `[ \"orange fruit orange\", \"fruit orange colour\", \"carrot vegetable orange\", \"vegetable orange colour\" ]` <br>\n",
    "So; all **4-grams** are: `[ \"orange fruit orange colour\", \"carrot vegetable orange colour\" ]` <br>\n",
    "So; after vectorizing using **bag of 1-grams** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 0 1 1 2 0 ], [ 1 1 0 1 1 ] ]` <br>\n",
    "So; after vectorizing using **bag of 2-grams** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 1 1 1 0 0 ], [ 0 0 1 1 1 ] ]` <br>\n",
    "So; after vectorizing using **bag of 3-grams** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 1 1 0 0 ], [ 0 0 1 1 ] ]` <br>\n",
    "So; after vectorizing using **bag of 4-grams** technique; the vectors corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`vectors = [ [ 1 0 ], [ 0 1 ] ]` <br><br>\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Adds some semantic relationship among the words than BOW, as now they are not only being considered independantly but also being considered in **consecutive word groups**, thereby somewhat relating the words.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Semantic relations are present, but are very weak. Any other relations are not present.\n",
    "- Weightage is given on basis of frequency in the sentence itself, so each sentence is considered independantly, no relation is considered between different sentences in entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ecbe7b4-d29f-432e-bf91-c8de40e4ef7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features(2-Grams) are:\n",
      "\n",
      "{   '3000 year': 0,\n",
      "    'alexander onwards': 1,\n",
      "    'british french': 2,\n",
      "    'came looted': 3,\n",
      "    'captured land': 4,\n",
      "    'come invaded': 5,\n",
      "    'conquered anyone': 6,\n",
      "    'conquered mind': 7,\n",
      "    'culture history': 8,\n",
      "    'done nation': 9,\n",
      "    'dutch came': 10,\n",
      "    'enforce way': 11,\n",
      "    'freedom others': 12,\n",
      "    'french dutch': 13,\n",
      "    'grabbed land': 14,\n",
      "    'greek turk': 15,\n",
      "    'history people': 16,\n",
      "    'history tried': 17,\n",
      "    'invaded captured': 18,\n",
      "    'land conquered': 19,\n",
      "    'land culture': 20,\n",
      "    'looted took': 21,\n",
      "    'mogul portuguese': 22,\n",
      "    'onwards greek': 23,\n",
      "    'people world': 24,\n",
      "    'portuguese british': 25,\n",
      "    'respect freedom': 26,\n",
      "    'three vision': 27,\n",
      "    'tried enforce': 28,\n",
      "    'turk mogul': 29,\n",
      "    'vision india': 30,\n",
      "    'way life': 31,\n",
      "    'world come': 32,\n",
      "    'year history': 33,\n",
      "    'yet done': 34}\n",
      "\n",
      "Features(3-Grams) are:\n",
      "\n",
      "{   '3000 year history': 0,\n",
      "    'alexander onwards greek': 1,\n",
      "    'british french dutch': 2,\n",
      "    'came looted took': 3,\n",
      "    'captured land conquered': 4,\n",
      "    'come invaded captured': 5,\n",
      "    'culture history tried': 6,\n",
      "    'dutch came looted': 7,\n",
      "    'enforce way life': 8,\n",
      "    'french dutch came': 9,\n",
      "    'grabbed land culture': 10,\n",
      "    'greek turk mogul': 11,\n",
      "    'history people world': 12,\n",
      "    'history tried enforce': 13,\n",
      "    'invaded captured land': 14,\n",
      "    'land conquered mind': 15,\n",
      "    'land culture history': 16,\n",
      "    'mogul portuguese british': 17,\n",
      "    'onwards greek turk': 18,\n",
      "    'people world come': 19,\n",
      "    'portuguese british french': 20,\n",
      "    'respect freedom others': 21,\n",
      "    'three vision india': 22,\n",
      "    'tried enforce way': 23,\n",
      "    'turk mogul portuguese': 24,\n",
      "    'world come invaded': 25,\n",
      "    'year history people': 26,\n",
      "    'yet done nation': 27}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "      <th>Bag of 2-GRAMS Vector Representation</th>\n",
       "      <th>Bag of 3-GRAMS Vector Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence  \\\n",
       "0                                                                     three vision india   \n",
       "1             3000 year history people world come invaded u captured land conquered mind   \n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took   \n",
       "3                                                                        yet done nation   \n",
       "4                                                                       conquered anyone   \n",
       "5                                    grabbed land culture history tried enforce way life   \n",
       "6                                                                                          \n",
       "7                                                                 respect freedom others   \n",
       "\n",
       "                                                                        Bag of 2-GRAMS Vector Representation  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]   \n",
       "1  [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]   \n",
       "2  [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "4  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]   \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                                   Bag of 3-GRAMS Vector Representation  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "1  [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0]  \n",
       "2  [0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "5  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop([\"Bag of words Vector Representation\"], axis=1, inplace=True)\n",
    "\n",
    "# Converting sentences to vectors using Bag Of N-Grams(here 3-grams) strategy\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(2,2))\n",
    "vectors = vectorizer.fit_transform(sentences).toarray().tolist()\n",
    "\n",
    "print(\"Features(2-Grams) are:\\n\")\n",
    "pp.pprint(vectorizer.vocabulary_)\n",
    "\n",
    "df[\"Bag of 2-GRAMS Vector Representation\"] = vectors\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(3,3))\n",
    "vectors = vectorizer.fit_transform(sentences).toarray().tolist()\n",
    "\n",
    "print(\"\\nFeatures(3-Grams) are:\\n\")\n",
    "pp.pprint(vectorizer.vocabulary_)\n",
    "\n",
    "df[\"Bag of 3-GRAMS Vector Representation\"] = vectors\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cc6c8-e609-48d0-b799-174cec582477",
   "metadata": {},
   "source": [
    "## <b><u>Term Frequency - Inverse Document Frequency (TF-IDF) Vectorizer</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534417f-84b8-4fb4-b84c-342b825bd8d4",
   "metadata": {},
   "source": [
    "In tf-idf technique also, we **consider WORDS as FEATURES (each unique word present in the entire corpus(collection of sentences) is treated as a unique feature)** <br>\n",
    "Each sentence is represented by a vector **based on the \"TF Score * IDF Score\" value of each of the unique words(features) present in the sentence** <br>\n",
    "\n",
    "**Term Frequency(TF) of a word W in a sentence S = (Frequency of W in S / Total number of words in S)** \n",
    "\n",
    "**Inverse Docment Frequency(IDF) of a word W in a corpus(collection of sentences) C = log(Total number of sentences in C / Number of sentences which contain W atleast once)**\n",
    "\n",
    "The **TF Score * IDF Score** value helps in **giving the more importent words more weightage and less important words less weightage**. <br>\n",
    "\n",
    "For Example: <br>\n",
    "Let `corpus = [ \"orange is a fruit of orange colour\", \"carrot is a vegetable of orange colour\" ]` <br>\n",
    "After cleanup and preprocessing 1, <br>\n",
    "`corpus = [ \"orange fruit orange colour\", \"carrot vegetable orange colour\" ]` <br>\n",
    "So; **unique words** in sorted order are: `[ \"carrot\", \"colour\", \"fruit\", \"orange\", \"vegetable\" ]` <br>\n",
    "So; the **TF vectors** corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`tf_vectors = [ [ 0 1/4 1/4 2/4 0 ], [ 1/4 1/4 0 1/4 1/4 ] ] = [ [ 0 0.25 0.25 0.5 0 ], [ 0.25 0.25 0 0.25 0.25 ] ]` <br>\n",
    "So; the **IDF vector** corresponding to the entire preprocessed corpus will be: <br>\n",
    "`idf_vector = [ log(2/1) log(2/2) log(2/1) log(2/2) log(2/1) ] = [ 0.3 0 0.3 0 0.3 ]` <br>\n",
    "So; the **TF-IDF vectors** corresponding to the sentences in the preprocessed corpus will be: <br>\n",
    "`tf_idf_vectors = [ [ (0*0.3) (0.25*0) (0.25*0.3) (0.5*0) (0*0.3) ], [ (0.25*0.3) (0.25*0) (0*0.3) (0.25*0) (0.25*0.3) ] ] = [ [ 0 0 0.075 0 0 ], [ 0.075 0 0 0 0.075 ] ]`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Why TF-IDF ?** <br>\n",
    "If we were to find the word which is most distinct among the two above sentences, which one would it be? <br>\n",
    "Note that, both the sentences talk about a substance which is orange in colour. So; the words \"orange\" and \"colour\" are present in both sentences and are not quite the distinguishing feature between them. So; these words are given less weightage (here). <br>\n",
    "The sentences do differ on the aspect that: The first sentence is about a \"fruit\" named \"apple\" while the second sentence is about a \"vegetable\" named \"carrot\". So; the words \"fruit\", \"vegetable\", \"apple\" and \"carrot\" are more important and help more in distinguishing the sentences. So; these words are given more weightage (here). <br>\n",
    "<br>\n",
    "So; **greater TF-IDF score => more distict feature(word)** and **lesser TF-IDF score => more common feature(word)** <br><br>\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Stronger semantic relationships between words than in bag of words/ngrams technique\n",
    "- Both relations between words in individual sentence (in TF score) as well as relations between words in different sentences (in IDF score) are considered. So; relations between sentences in entire corpus being considered.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- No other relations except semantic relations being considered.\n",
    "- The semantic relations are stronger than bow/bon but they are still weak (can be made better as we see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "035a1a7d-23ba-4abf-bab8-60e0a8db0bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features(Words) are:\n",
      "\n",
      "['3000' 'alexander' 'anyone' 'british' 'came' 'captured' 'come'\n",
      " 'conquered' 'culture' 'done' 'dutch' 'enforce' 'freedom' 'french'\n",
      " 'grabbed' 'greek' 'history' 'india' 'invaded' 'land' 'life' 'looted'\n",
      " 'mind' 'mogul' 'nation' 'onwards' 'others' 'people' 'portuguese'\n",
      " 'respect' 'three' 'took' 'tried' 'turk' 'vision' 'way' 'world' 'year'\n",
      " 'yet']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "      <th>TF-IDF Vector Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "      <td>[0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.31454746818160906, 0.2636153271241494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636153271241494, 0.0, 0.31454746818160906, 0.2636153271241494, 0.0, 0.0, 0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.31454746818160906, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "      <td>[0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "      <td>[0.0, 0.0, 0.7664298449085388, 0.0, 0.0, 0.0, 0.0, 0.6423280258820045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.30798479381600735, 0.0, 0.0, 0.30798479381600735, 0.36748939521121393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence  \\\n",
       "0                                                                     three vision india   \n",
       "1             3000 year history people world come invaded u captured land conquered mind   \n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took   \n",
       "3                                                                        yet done nation   \n",
       "4                                                                       conquered anyone   \n",
       "5                                    grabbed land culture history tried enforce way life   \n",
       "6                                                                                          \n",
       "7                                                                 respect freedom others   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                              TF-IDF Vector Representation  \n",
       "0                                                                                                                                         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0]  \n",
       "1         [0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.31454746818160906, 0.2636153271241494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2636153271241494, 0.0, 0.31454746818160906, 0.2636153271241494, 0.0, 0.0, 0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31454746818160906, 0.31454746818160906, 0.0]  \n",
       "2  [0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, 0.2886751345948129, 0.0, 0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3                                                                                                                                         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258]  \n",
       "4                                                                                                                                                        [0.0, 0.0, 0.7664298449085388, 0.0, 0.0, 0.0, 0.0, 0.6423280258820045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "5                                                      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.30798479381600735, 0.0, 0.0, 0.30798479381600735, 0.36748939521121393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.36748939521121393, 0.0, 0.0, 0.0]  \n",
       "6                                                                                                                                                                                      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "7                                                                                                                                         [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting sentences to vectors using TF-IDF strategy\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sentences).toarray().tolist()\n",
    "\n",
    "print(\"Features(Words) are:\\n\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "df.drop([\"Bag of 2-GRAMS Vector Representation\"], axis=1, inplace=True)\n",
    "df.drop([\"Bag of 3-GRAMS Vector Representation\"], axis=1, inplace=True)\n",
    "df[\"TF-IDF Vector Representation\"] = vectors\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc5099-6eb8-427c-82bc-1c3e99378f91",
   "metadata": {},
   "source": [
    "## <b><u>Hash Vectorizer</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb39b20-7f94-49f5-942c-b2f932a1227f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed Sentence</th>\n",
       "      <th>Hashed Vector Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three vision india</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000 year history people world come invaded u captured land conquered mind</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alexander onwards greek turk mogul portuguese british french dutch came looted u took</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yet done nation</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conquered anyone</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grabbed land culture history tried enforce way life</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>respect freedom others</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Preprocessed Sentence  \\\n",
       "0                                                                     three vision india   \n",
       "1             3000 year history people world come invaded u captured land conquered mind   \n",
       "2  alexander onwards greek turk mogul portuguese british french dutch came looted u took   \n",
       "3                                                                        yet done nation   \n",
       "4                                                                       conquered anyone   \n",
       "5                                    grabbed land culture history tried enforce way life   \n",
       "6                                                                                          \n",
       "7                                                                 respect freedom others   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Hashed Vector Representation  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting sentences to vectors using hashing strategy\n",
    "vectorizer = sklearn.feature_extraction.text.HashingVectorizer()\n",
    "vectors = vectorizer.fit_transform(sentences).toarray().tolist()\n",
    "\n",
    "df.drop([\"TF-IDF Vector Representation\"], axis=1, inplace=True)\n",
    "df[\"Hashed Vector Representation\"] = vectors\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d201b-f882-4868-b89a-75cab990741f",
   "metadata": {},
   "source": [
    "## <b><u>Word Embedding: Finding even more relationships among different words in the vocabulary</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f399e68-1417-4b94-a628-2fa365d45e86",
   "metadata": {},
   "source": [
    "Considering each word as a vector, so each sentence is 2D matrix, the corpus is 3D matrix...\n",
    "\n",
    "Word2Vec, fasttext, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c11b4-5c4e-41a4-9190-8b71017d27f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
