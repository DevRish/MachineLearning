{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b5b63f-ed5f-44c8-93b2-a8c66947573c",
   "metadata": {},
   "source": [
    "# <b><u>Evaluation using Word Embedding : Word2Vec</u></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c89d5-fb60-4e04-95d9-d2ab5afa289d",
   "metadata": {},
   "source": [
    "One of the main drawbacks in BLEU is no detection of proper grammar or appropriate placement / usage of words. <br>\n",
    "The aim here is to deal with that by introducing features through word embedding, based on which we can determine how well a word is placed and related with the other words in the sentence. <br>\n",
    "The reference sentences are correctly translated sentences. So; using the corpus of all reference sentences, a word embedding model is trained.<br>\n",
    "After that, we have a vocabulary of words from the reference corpus, along with feature vectors for each word that represents it's relations with other words.<br>\n",
    "Using these vectors, we try to find some evaluation score that may work better than BLEU in terms of semantics.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f70772-6b67-4879-91e9-4e2de6b607ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d929609f-c97a-4c88-84fa-4ed11d917fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       Hi.\n",
       "1                                                      Run!\n",
       "2                                                      Run!\n",
       "3                                                      Who?\n",
       "4                                                      Wow!\n",
       "                                ...                        \n",
       "175616    Top-down economics never works, said Obama. \"T...\n",
       "175617    A carbon footprint is the amount of carbon dio...\n",
       "175618    Death is something that we're often discourage...\n",
       "175619    Since there are usually multiple websites on a...\n",
       "175620    If someone who doesn't know your background sa...\n",
       "Name: English, Length: 175621, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"eng-french.csv\")\n",
    "df.columns = [\"English\",\"French\"]\n",
    "df = df[\"English\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779d577-9ff7-4464-bdc8-8ce04a66760c",
   "metadata": {},
   "source": [
    "### <u><b>Preprocessing:</b></u>\n",
    "\n",
    "- All words converted to lower case, so that words are not treated uniquely just for case difference. Example: The word \"school\" will be written as \"School\" if present at the start of a sentence. But \"School\" and \"school\" mean the same thing.\n",
    "- All punctuations are removed as they don't play a vital role in English (might not be the case for other languages)\n",
    "- Stopwords are NOT removed as their consideration does matter here. A sentence translated with stopwords missing will be a badly translated sentence. Stopwords play a considerable role in the quality of translation.\n",
    "- Words are lemmatized to reduce number of unique words for convinience in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b93b21-a4f1-4add-a827-aef9f7df6198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      [hi]\n",
       "1                                                     [run]\n",
       "2                                                     [run]\n",
       "3                                                     [who]\n",
       "4                                                     [wow]\n",
       "                                ...                        \n",
       "175616    [topdown, economics, never, work, said, obama,...\n",
       "175617    [a, carbon, footprint, is, the, amount, of, ca...\n",
       "175618    [death, is, something, that, were, often, disc...\n",
       "175619    [since, there, are, usually, multiple, website...\n",
       "175620    [if, someone, who, doesnt, know, your, backgro...\n",
       "Name: English, Length: 175621, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(x):\n",
    "    s = x.lower() # convert all to lower case for convenience\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]+', '', s) # remove punctuation\n",
    "    s = s.split(\" \")\n",
    "    s = [lemmatizer.lemmatize(word) for word in s ] # lemmatize to base words\n",
    "    # stop words not removed as here they are part of language and their presence(along with location) does matter\n",
    "    return s\n",
    "\n",
    "preprocessed_sentences = df.apply(lambda x: preprocess(x))\n",
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c3708-fd4c-459d-bab9-dc56b58211db",
   "metadata": {},
   "source": [
    "### <u><b>Training the Word2Vec model:</b></u>\n",
    "\n",
    "- `window` => The number of words on both left and right side to which the current word is to be related with (bigger -> better)\n",
    "- `vector_size` => The number of `features` that the word embedding model will generate for relating the words (bigger -> better)\n",
    "- `min_count` => Minimum number of words required to be present in a sentence for it to be considered for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e3eb9d-6a45-454f-a45e-3495b5e3f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3754136, 5410400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(window=10, vector_size=2000, min_count=2, workers=4)\n",
    "model.build_vocab(preprocessed_sentences)\n",
    "model.train(preprocessed_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a282afb-3e3f-4227-b535-f25e05f625a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary:  8810\n",
      "Size(number of features) of each word vector:  2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in vocabulary: \", len(model.wv))\n",
    "print(\"Size(number of features) of each word vector: \", model.vector_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f51517-3e0f-4b7b-81f5-b262ac87fcfc",
   "metadata": {},
   "source": [
    "### <u><b>Some points to note:</b></u>\n",
    "\n",
    "- The number of words in `translation` may not be equal to the `number of words` in `reference`. So; if we just put the 1D vectors of each of them together, the 2D matrix generated for `translation` may not have the same `number of columns` as the 2D matrix generated for `reference`. However, the `number of rows` in both matrices will be equal and they will be equal to the `number of features` word embedding generated.\n",
    "- All words that are present in the `translation` may not even be present in the `reference` corpus's `vocabulary`, even if they are a part of the language(dictionary).\n",
    "- Position of the word in the sentence needs to be emphasized. Some languages require specific parts of speech to be at beginning, some at middle, some at end. For example, in English the order is: `Subject-Verb-Object`, however, in Japanese the order is `Subject-Object-Verb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb6718-e3eb-463c-ba0e-43222e1c8c61",
   "metadata": {},
   "source": [
    "### <u><b>Some test data used to compare the performances:</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c7f43d-6d13-4196-8185-a9ae9b0e9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"I go to the school.\"\n",
    "translation1 = \"I go to school.\"\n",
    "translation2 = \"I go school.\"\n",
    "translation3 = \"I go.\"\n",
    "translation4 = \"I school the go.\"\n",
    "\n",
    "# Pre-process all\n",
    "reference = preprocess(reference)\n",
    "translation1 = preprocess(translation1)\n",
    "translation2 = preprocess(translation2)\n",
    "translation3 = preprocess(translation3)\n",
    "translation4 = preprocess(translation4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334397b-f5e8-4ee6-90bb-da150e2d5ecd",
   "metadata": {},
   "source": [
    "### <u><b>Evaluation Logic 1:</b></u>\n",
    "\n",
    "- To address the issue of varying number of columns, we can add up the feature values of all words thereby generating a single `n_feature x 1` vector for each of translation and reference\n",
    "- Simple addition wouldn't account for the word's position, so take a weighted sum by position to give importance to position\n",
    "- So; for each of translation and reference; find `sentence_vector = sum(position * word_vector)`\n",
    "- Finally, compare the two resultant `n_feature x 1` dimensional `sentence_vectors` using various methods of comparing vectors. Here; `cosine similarity` used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1967c351-8321-40d3-be74-4e375afa0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_count = model.vector_size\n",
    "\n",
    "def find_score_1(reference, translation):\n",
    "    # Filter Out of Vocabulary words from translation\n",
    "    tran = [word for word in translation if word in list(model.wv.index_to_key)]\n",
    "    ref_val = np.zeros(f_count) \n",
    "    tran_val = np.zeros(f_count)\n",
    "    for i in range(len(reference)):\n",
    "        ref_val = ref_val + ((i+1)*model.wv[reference[i]])\n",
    "    \n",
    "    for i in range(len(tran)):\n",
    "        tran_val = tran_val + ((i+1)*model.wv[tran[i]])\n",
    "\n",
    "    cos_sim = np.dot(ref_val,tran_val)/(np.linalg.norm(ref_val)*np.linalg.norm(tran_val))\n",
    "    return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84adafd9-2708-40bb-8f5d-74b5bead0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For translation1:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.8900996736071406 \n",
      "\n",
      "For translation2:\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "Experimental Score:  0.7969980684799112 \n",
      "\n",
      "For translation3:\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "Experimental Score:  0.5344254095203471 \n",
      "\n",
      "For translation4:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.8125226141641856 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishav_2022/Desktop/MachineLearning/ml-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/rishav_2022/Desktop/MachineLearning/ml-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/rishav_2022/Desktop/MachineLearning/ml-env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"For translation1:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation1))\n",
    "print(\"Experimental Score: \", find_score_1(reference,translation1), \"\\n\")\n",
    "\n",
    "print(\"For translation2:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation2))\n",
    "print(\"Experimental Score: \", find_score_1(reference,translation2), \"\\n\")\n",
    "\n",
    "print(\"For translation3:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation3))\n",
    "print(\"Experimental Score: \", find_score_1(reference,translation3), \"\\n\")\n",
    "\n",
    "print(\"For translation4:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation4))\n",
    "print(\"Experimental Score: \", find_score_1(reference,translation4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c276-609a-4c9a-8d68-0ffee487a70e",
   "metadata": {},
   "source": [
    "### <u>Observations for this evaluation Score:</u>\n",
    "\n",
    "- When words are decreased, BLEU score tends to increase wrongly(due to reduction in number of non-matching n-grams), but new evaluation score decreases correctly.\n",
    "- `translation4` is more scrambled up than `translation2` but BLEU score gives both the same score as both contain eqal number of common words w.r.t. `reference`. But new evaluation score correctly identifies the difference and gives the scrambled up version lesser score and less scrambled version more score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd96a17-c6b4-4302-9363-c03931f66333",
   "metadata": {},
   "source": [
    "### <u><b>Evaluation Logic 2:</b></u>\n",
    "\n",
    "- To address both the issue of varying number of columns and the importance of position together, consider the missing columns in `translation`'s sentence vector as zeroes\n",
    "- Finally, compare the two resultant `number of feaures x number of words in reference sentence` dimensional `sentence_vectors` using various methods of comparing vectors. Here; `cosine similarity` used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44ce13e-78d6-4217-a07d-62fa2353e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_count = model.vector_size\n",
    "\n",
    "def find_score_2(reference, translation):\n",
    "    ref_vecs = []\n",
    "    tran_vecs = []\n",
    "    for i in range(len(reference)):\n",
    "        ref_vecs.append(model.wv[reference[i]])\n",
    "    \n",
    "    for i in range(len(translation)):\n",
    "        if translation[i] in reference:\n",
    "            tran_vecs.append(model.wv[translation[i]])\n",
    "        else:\n",
    "            tran_vecs.append(np.zeros(f_count))\n",
    "\n",
    "    while len(tran_vecs) < len(ref_vecs):\n",
    "        tran_vecs.append(np.zeros(f_count))\n",
    "\n",
    "    # ref_matrix = np.stack(ref_vecs, axis=0)\n",
    "    # print(ref_matrix.shape)\n",
    "    # tran_matrix = np.stack(tran_vecs, axis=0)\n",
    "    # print(ref_matrix.shape)\n",
    "\n",
    "    # Flatten\n",
    "    ref_vec = np.hstack(ref_vecs)\n",
    "    tran_vec = np.hstack(tran_vecs)\n",
    "    \n",
    "    cos_sim = np.dot(ref_vec,tran_vec)/(np.linalg.norm(ref_vec)*np.linalg.norm(tran_vec))\n",
    "    return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1949a6-ea49-4705-bab7-3a173f48cf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For translation1:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.7842435191132129 \n",
      "\n",
      "For translation2:\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "Experimental Score:  0.6434229198926371 \n",
      "\n",
      "For translation3:\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "Experimental Score:  0.7127484462590815 \n",
      "\n",
      "For translation4:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.23147301424872077 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For translation1:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation1))\n",
    "print(\"Experimental Score: \", find_score_2(reference,translation1), \"\\n\")\n",
    "\n",
    "print(\"For translation2:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation2))\n",
    "print(\"Experimental Score: \", find_score_2(reference,translation2), \"\\n\")\n",
    "\n",
    "print(\"For translation3:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation3))\n",
    "print(\"Experimental Score: \", find_score_2(reference,translation3), \"\\n\")\n",
    "\n",
    "print(\"For translation4:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation4))\n",
    "print(\"Experimental Score: \", find_score_2(reference,translation4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0908a4f-00b3-4cf7-8035-f3a785c41e4c",
   "metadata": {},
   "source": [
    "### <u>Observations for this evaluation Score:</u>\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7650804-8f8a-4da2-996d-bfa29fdeb779",
   "metadata": {},
   "source": [
    "### <u><b>Evaluation Logic 3:</b></u>\n",
    "\n",
    "- For each word(word_vector), take sum of it's cosine similarities with adjacent words and store that in position of the word. Thus; constructing a vector for the entire sentence.\n",
    "- Find similarity between the sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8053bccd-d3f5-4fdf-ae21-e4b67db07741",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_count = model.vector_size\n",
    "\n",
    "def find_score_3(reference, translation):\n",
    "    ref_vec = []\n",
    "    tran_vec = []\n",
    "    for i in range(len(reference)):\n",
    "        total_sim = 0\n",
    "        if i-1 >= 0:\n",
    "            wvec_1 = model.wv[reference[i-1]]\n",
    "            wvec_2 = model.wv[reference[i]]\n",
    "            cs = np.dot(wvec_1,wvec_2)/(np.linalg.norm(wvec_1)*np.linalg.norm(wvec_2))\n",
    "            total_sim += cs\n",
    "        if i+1 < len(reference):\n",
    "            wvec_1 = model.wv[reference[i]]\n",
    "            wvec_2 = model.wv[reference[i+1]]\n",
    "            cs = np.dot(wvec_1,wvec_2)/(np.linalg.norm(wvec_1)*np.linalg.norm(wvec_2))\n",
    "            total_sim += cs\n",
    "        ref_vec.append(total_sim)\n",
    "    \n",
    "    for i in range(len(translation)):\n",
    "        total_sim = 0\n",
    "        if i-1 >= 0:\n",
    "            wvec_1 = model.wv[translation[i-1]]\n",
    "            wvec_2 = model.wv[translation[i]]\n",
    "            cs = np.dot(wvec_1,wvec_2)/(np.linalg.norm(wvec_1)*np.linalg.norm(wvec_2))\n",
    "            total_sim += cs\n",
    "        if i+1 < len(translation):\n",
    "            wvec_1 = model.wv[translation[i]]\n",
    "            wvec_2 = model.wv[translation[i+1]]\n",
    "            cs = np.dot(wvec_1,wvec_2)/(np.linalg.norm(wvec_1)*np.linalg.norm(wvec_2))\n",
    "            total_sim += cs\n",
    "        tran_vec.append(total_sim)\n",
    "\n",
    "    while len(tran_vec) < len(ref_vec):\n",
    "        tran_vec.append(0)\n",
    "\n",
    "    # ref_matrix = np.stack(ref_vecs, axis=0)\n",
    "    # print(ref_matrix.shape)\n",
    "    # tran_matrix = np.stack(tran_vecs, axis=0)\n",
    "    # print(ref_matrix.shape)\n",
    "\n",
    "    # Flatten\n",
    "    ref_vec = np.array(ref_vec)\n",
    "    tran_vec = np.array(tran_vec)\n",
    "    \n",
    "    cos_sim = np.dot(ref_vec,tran_vec)/(np.linalg.norm(ref_vec)*np.linalg.norm(tran_vec))\n",
    "    return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c53c1316-463c-4321-9664-82e219e063c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For translation1:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.8768803745721149 \n",
      "\n",
      "For translation2:\n",
      "BLEU Score:  1.384292958842266e-231\n",
      "Experimental Score:  0.7817830949725829 \n",
      "\n",
      "For translation3:\n",
      "BLEU Score:  1.5319719891192393e-231\n",
      "Experimental Score:  0.8196272693811495 \n",
      "\n",
      "For translation4:\n",
      "BLEU Score:  1.2882297539194154e-231\n",
      "Experimental Score:  0.7913911273644312 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For translation1:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation1))\n",
    "print(\"Experimental Score: \", find_score_3(reference,translation1), \"\\n\")\n",
    "\n",
    "print(\"For translation2:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation2))\n",
    "print(\"Experimental Score: \", find_score_3(reference,translation2), \"\\n\")\n",
    "\n",
    "print(\"For translation3:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation3))\n",
    "print(\"Experimental Score: \", find_score_3(reference,translation3), \"\\n\")\n",
    "\n",
    "print(\"For translation4:\")\n",
    "print(\"BLEU Score: \", nltk.translate.bleu_score.sentence_bleu(reference,translation4))\n",
    "print(\"Experimental Score: \", find_score_3(reference,translation4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053f300-33db-40cd-8475-3bac9efa018e",
   "metadata": {},
   "source": [
    "### <u>Observations for this evaluation Score:</u>\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b7ac1-0a8c-4e31-9f3c-fab42c05a45b",
   "metadata": {},
   "source": [
    "### <u><b>Moving Parts:</b></u>\n",
    "\n",
    "- The accuracy of the word embedding model and scheme determine the word vectors. So; they play a crucial role.\n",
    "    - Try different word embedding techniques: word2vec, fasttext, glove, see which performs best\n",
    "    - Try experimenting with different parameters of each model\n",
    "- How we use the word vectors for finding a evaluation score is important\n",
    "    - Try other ways of finding similarity between vectors other than cosine similarity\n",
    "    - Compare the results of different similarity finding mechanisms\n",
    "    - Find other evaluation methods on 1D vectors as well as 2D matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cce559-e1da-46f7-985e-fbc26d71c3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
